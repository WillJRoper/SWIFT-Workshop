{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed4c9ba-ca27-4da9-abfb-eddfdc5d2247",
   "metadata": {},
   "source": [
    "# Using A Complete Physics Model\n",
    "\n",
    "It's time to dispense with these toy simulations and do something real! Luckily, this time round we don't need to generate initial conditions. SWIFT comes packaged with numerous example initial conditions we could use. From the Santa Barbara simulations, which were used to show the consistency of CDM with our Universe decades ago, to the more modern EAGLE simulation suite. We'll be using the smallest EAGLE box at low $z$ which can be found in `SWIFT/examples/EAGLE_low_z/EAGLE_6/` and running it with the SWIFT implementation of the EAGLE model ([Borrow+22]()).\n",
    "\n",
    "## Compiling for a full model\n",
    "\n",
    "Before we can dive in and run this simulation we need to again reconfigure and compile to turn on all of the extra physics we will need. Thankfully, instead of turning on each individual part of the model, SWIFT allows us to just tell it which sub-grid model we want to use at configure time. To configure with the EAGLE model simply run the following in the `SWIFT` directory.\n",
    "```\n",
    "./configure --disable-mpi --disable-compiler-warnings --disable-doxygen-doc --disable-hand-vec --with-subgrid=EAGLE-XL --with-hydro=sphenix --with-kernel=wendland-C2\n",
    "```\n",
    "(here I've again included the Mac caveat options, but you may not need to). Now for the final time invoke `make` in the SWIFT directory, you now have an all singing all dancing version of SWIFT available in the `swift` executable.\n",
    "\n",
    "I've included a slightly modified parameter file for this simulation in this repository so you don't need to get the one in the `examples` folder, but we do need some extra files that the EAGLE model is dependent on. To get these navigate back to the `SWIFT-Workshop` directory and run:\n",
    "```\n",
    "/path/to/SWIFT/examples/EAGLE_ICs/getEaglePhotometryTable.sh\n",
    "/path/to/SWIFT/examples/EAGLE_ICs/getPS2020CoolingTables.sh\n",
    "/path/to/SWIFT/examples/EAGLE_ICs/getEagleYieldTable.sh\n",
    "```\n",
    "\n",
    "The names of these scripts are a bit of a giveaway as to what they are used for. The former is used in the calculation of on-the-fly line-of-sight photometry in GAMA bands, the second is a file containing cooling tables that define the equation of state for the interstellar medium, and the latter is a table of elemental yields from supernovae.\n",
    "\n",
    "The final thing we need to do is download the ICs themselves. To do so enter the following into the terminal.\n",
    "```\n",
    "cd ics\n",
    "./EAGLE6_getIC.sh\n",
    "cd ../\n",
    "```\n",
    "\n",
    "## Running the simulation\n",
    "\n",
    "Ensure you are in the base directory of the `SWIFT-Workshop` repository (the file paths in the parameter file to the dependent files we downloaded above assume this is where you are running) and we can start the simulation running. The call is a bit different to the call we've been using up until now. The flags we have previously been using are actually implicitly part of the `--eagle` flag which also tells SWIFT to run with all the physics models contained within EAGLE. We have also finally turned on cosmology via the `--cosmology` flag.\n",
    "```\n",
    "/path/to/SWIFT/swift --cosmology --eagle --threads <n> params/eagle_6.yml\n",
    "```\n",
    "\n",
    "This simulation will take longer than the previous simulations but it'll be worth it. Once we have a few snapshots we can move on to the subsequent cells.\n",
    "\n",
    "# Analysing simulations\n",
    "\n",
    "We now have at least one snapshot containing all the properties of all particles at the time of output. We can now go into these snapshots and plot some interesting relations. \n",
    "\n",
    "This is where the hand-holding stops and we're going to make you work. Recall the reading of HDF5 files and their structure:\n",
    "- `h5py.File` with the `\"r\"` flag to read the file.\n",
    "- The dictionary key structure for accessing arrays stored within.\n",
    "- The slicing and data extraction using the `[:]` syntax.\n",
    "- The particle types names: `\"PartType0\"` for gas, `\"PartType1\"` for dark matter, `\"PartType4\"` for stars, and `\"PartType5\"` for black holes.\n",
    "\n",
    "Using the cell below and what you've seen in previous notebooks explore the snapshot (you can use `hdf[<key>].keys()` to see the fields stored at a particular key/in a group, or view the whole file using `h5glance` [see below]) and look at what fields each particle type has. Note that every single field in SWIFT snapshots comes with descriptive metadata attached, see [the docs](https://swift.strw.leidenuniv.nl/docs/Snapshots/index.html#unit-information-for-individual-fields) for more information.\n",
    "\n",
    "## `h5glance`\n",
    "\n",
    "We can install and run a nice command line tool for probing HDF5 files. Jupyter notebooks let you run code on the command line via the `!` operator. Let's look at an IC file for demonstration. This will show you a key structure of all the fields contained within the file under thier parent \"Groups\" (given by the parent key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2eeae-691a-4f04-975e-09bfa79db206",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install h5glance\n",
    "!h5glance ../ics/EAGLE_ICs_6.hdf5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d32610-3933-4329-8829-c46c8af11e59",
   "metadata": {},
   "source": [
    "## Plotting particle properties\n",
    "\n",
    "First we can look at a phase diagram of the entire simulated volume (we could do this later for individual halos which would be much more useful). Being able to go in a prove the properties of individual particles (mass elements) in specific environments is one of the main strengths of simulations.\n",
    "\n",
    "The code below will extract the temperature and density of all gas particles in the box and plot them. (Remember to change the snapshot to the latest snapshot your simulation has reached.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4a349a-acba-46ce-b82b-e9a2706fd960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Extract the temperatures and densities\n",
    "hdf = h5py.File(\"../snapshots/EAGLE_6/snapshot_0002.hdf5\", \"r\")\n",
    "dens = hdf[\"PartType0/Densities\"][:] * hdf[\"PartType0/Densities\"].attrs[\"Conversion factor to physical CGS (including cosmological corrections)\"]\n",
    "temps = hdf[\"PartType0/Temperatures\"][:]\n",
    "hdf.close()\n",
    "\n",
    "# See how we applied the correction to physical cgs to the density stored in the SWIFT snapshot. Simple!\n",
    "\n",
    "# Plot the phase diagram\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.hexbin(dens, temps, mincnt=1, linewidths=0.2, cmap=\"plasma\", xscale=\"log\", yscale=\"log\", norm=LogNorm())\n",
    "ax.set_xlabel(\"Density / [ g / cm$^3$]\")\n",
    "ax.set_ylabel(\"Temperature / [K]\")\n",
    "cbar = fig.colorbar(im)\n",
    "cbar.set_label(\"$N$\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05db186a-351c-4e63-a516-b55a6991352a",
   "metadata": {},
   "source": [
    "Can you plot a cosmic star formation rate density plot from the stars? (Make sure you're using the most recent snapshot, this plot will be aided by having run the simulation for a bit to get more snapshots.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d308e-504f-4465-8241-72e0a792fe09",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>HINT</summary>\n",
    "The cosmic star formation rate density describes the star formation rate of the entire universe as a function of redshift. The birth scale factor of each star is stored under `\"PartType4/BirthScaleFactors\"`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ceb0a-b4af-446b-b19e-1a3af5c67a75",
   "metadata": {},
   "source": [
    "## Extracting structure\n",
    "\n",
    "We now have a real cosmological simulation with all the bells and whistles but the only outputs we have are big arrays of numbers for every particle in the simulation. As already demonstrated you can compute things of interest from these arrays but really we want halos and galaxies. So, how do we go from a sea of particles to identified structures...\n",
    "\n",
    "The answer is (drum roll please...) halo finders. There exist a myriad of structure/halo finders, each with their own strengths and weaknesses. Here we will use the simplest possible version: a spatial Friends-of-Friends (FOF) algorithm. \n",
    "\n",
    "This class of algorithms find all the neighbours of each particle and then marries all neighbour collections with particles in common. (If you care about the technical nomenclature we'll be use a union-find type algorithm for the marrying stage.) To find neighbours we need a neighbour definition, so what makes two particles neighbours? \n",
    "\n",
    "### The linking length\n",
    "\n",
    "In a FOF any two particles are neighbours if they are separated by less than the \"linking length\" ($\\ell$). For host halos (the size of halos corresponding to galaxy groups and clusters) this length is well-defined,\n",
    "$$\n",
    "\\ell = 0.2 \\bar{x},\n",
    "$$\n",
    "where $\\bar{x}$ is again the mean inter-particle separation and the 0.2 corresponds to structures around 200 times the mean density of the Universe. We'll begin by defining this linking length and using `scipy` to construct a KD-Tree from the positions of dark matter particles. A KD-Tree is an efficient method for finding neighbours without computing the distance to all other particles for every particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b16177-4bed-43ee-b39d-acfff7569573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import astropy.units as u\n",
    "from astropy.cosmology import Planck18 as cosmo\n",
    "from scipy.spatial import cKDTree\n",
    "\n",
    "\n",
    "# First let's read the dark matter positions and masses from the most recent snapshot (edit the file path to match yours)\n",
    "hdf = h5py.File(\"../snapshots/EAGLE_6/snapshot_0002.hdf5\", \"r\")\n",
    "pos = hdf[\"PartType1/Coordinates\"][:]\n",
    "masses = hdf[\"PartType1/Masses\"][:] * 10 ** 10\n",
    "hdf.close()\n",
    "\n",
    "# Calculate the mean interparticle separation\n",
    "xbar = (1 / (cosmo.Om(0) * cosmo.critical_density(0) / masses[0] / u.Msun)) ** (1/3)\n",
    "\n",
    "# Calculate the linking length\n",
    "linkl = 0.2 * xbar.to(u.Mpc).value\n",
    "\n",
    "print(\"linkl =\", linkl)\n",
    "\n",
    "# Construct the kd-tree\n",
    "tree = cKDTree(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b851c392-58b1-41a6-9f5b-ba77db11277c",
   "metadata": {},
   "source": [
    "With the linking length calculated and KD-Tree constructed all we have to do to find the dark matter halos is query the tree with the position of every dark matter particle. This will produce a list of lists where the list at index `j` holds the indices of all neighbours of particle `j`. **This may take a moment!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a0056-b381-4170-b7fd-ede8b62aedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inds = tree.query_ball_point(pos, r=linkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f4e57-1be2-4a88-8743-56cab782f348",
   "metadata": {},
   "source": [
    "We can now combine each of these individual results such that each particle points to all its neighbours in an array. Again this may take a while. Single-threaded Python is not the ideal tool for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201fde4-6e4b-4356-b047-450511c21295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dict to hold the halos\n",
    "ini_halo_indices = np.full(pos.shape[0], set())\n",
    "\n",
    "# Loop over particle entries\n",
    "for j, parts in enumerate(n_inds):\n",
    "    print(j, end=\"\\r\")\n",
    "    \n",
    "    # Skip neighbourless particles\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "\n",
    "    # Get the neighbouring particle's existing sets\n",
    "    nbour_sets = ini_halo_indices[parts]\n",
    "\n",
    "    # Combine the sets\n",
    "    nbour_sets[0].union(*nbour_sets[1:])\n",
    "\n",
    "    # Update the set with the current neighbours\n",
    "    nbour_sets[0].update(set(parts))\n",
    "\n",
    "    # Update the sets each particle points to\n",
    "    ini_halo_indices[parts] = nbour_sets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86b9a00-5c30-4e07-8091-1b711872b4c6",
   "metadata": {},
   "source": [
    "And with this array in hand we can now extract each individual halo and store each halo's particles in a dictionary to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b433faa-e535-43c8-824c-00603b514aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have all particles related we can create the final halo arrays...\n",
    "halo_indices = {}\n",
    "\n",
    "# ... to do this we will associate the halos to the particle with the minimum index.\n",
    "for j, parts in enumerate(ini_halo_indices):\n",
    "    print(j, end=\"\\r\")\n",
    "    \n",
    "    # Skip neighbourless particles\n",
    "    if len(parts) < 2:\n",
    "        continue\n",
    "\n",
    "    # Get the minimum particle \n",
    "    min_ind = min(parts)\n",
    "\n",
    "    # Attach these particles to that key\n",
    "    if min_ind in halo_indices:\n",
    "        continue\n",
    "    halo_indices[min_ind] = parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656f0c1-ca61-43b4-bf97-80c5b3502675",
   "metadata": {},
   "source": [
    "We now have a dictionary of all the indices for each halo. Lets see how many halos we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de43bac9-39af-4d01-8e64-4aba76962eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N_halo = \", len(halo_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3a37f7-42cf-46e4-9bf9-17ec33d9b664",
   "metadata": {},
   "source": [
    "This is quite a few halos for such a small volume. In fact, we have made a fundamental error here. The minimum number of particles in a halo we have in the above is 2! That's not very well resolved! Best practice is to use a minimum number larger than 20. Here we will use 32 to match the original halo catalogue from EAGLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4020167-f320-4193-bae4-f4deb30eee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the halo catalogue removing halos below the minimum particle count, convert the values to arrays\n",
    "# and let's make some nice sequential halo IDs while we are at it\n",
    "halo_id = 0\n",
    "keys = list(halo_indices.keys())\n",
    "values = list(halo_indices.values())\n",
    "for key, parts in zip(keys, values):\n",
    "    \n",
    "    del halo_indices[key]\n",
    "    if len(parts) >= 32:\n",
    "        halo_indices[halo_id] = np.array(list(parts))\n",
    "        halo_id += 1\n",
    "\n",
    "print(\"N_halo = \", halo_id)\n",
    "\n",
    "# Define how many halos we have\n",
    "nhalo = halo_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8205f84d-379e-4043-9c8c-6c245654ce37",
   "metadata": {},
   "source": [
    "That's much more reasonable! There was a lot of statistical nonsense in our catalogue before!\n",
    "\n",
    "### Caveats\n",
    "\n",
    "In reality, spatial FOFs perform fairly horribly. They're very good (due to their simplicity) for on-the-fly calculations which only need quick estimates of structure. The `--eagle` flag we used above also turns on SWIFT's spatial FOF for various calculations used in the physics model such as black hole seeding. However, they do not produce robust structures, they'll abundantly return statistical nonsense at the lowest masses while sometimes producing unbound and nonsensical structures at higher masses, particularly during mergers. \n",
    "\n",
    "Halo finders, even if they start with a spatial FOF initially, employ numerous methods to produce robust structures. They'll use unbinding steps to remove unbound particles, some refine in phase space using velocity information, and others use the matter field to construct spherical overdensities, and this is just for the host halo level. Subhalos, the halos which contain galaxies, are not well described by a single linking length. Halo finders instead use other methods to identify these within host halos, some in phase space iteratively or recursively, others by finding saddle points in the density field, and as always each approach has its foibles. What we have done (and will do later) is a GROSS simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ff752-a2ea-484a-a2e4-4d42138f9835",
   "metadata": {},
   "source": [
    "## Plotting dark matter halo properties\n",
    "\n",
    "To actually plot the halo properties from the dictionary created above we can simply index the particle property arrays with the indices of the particles in each halo. For instance, to calculate the dark matter mass of each halo we can simply do the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e717e1-2ce1-4e8f-849c-d0fe94a1d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create array to store halo massses\n",
    "dm_halo_ms = np.zeros(nhalo)\n",
    "\n",
    "# Loop over halos calculating DM mass\n",
    "for i in range(nhalo):\n",
    "    dm_halo_ms[i] = np.sum(masses[halo_indices[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98112be5-a35b-466d-a34c-1b48927fffc8",
   "metadata": {},
   "source": [
    "Can you plot a halo mass function in the cell below from this array of masses?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490663a-85ce-47a2-96b3-45be312eeb54",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>HINT</summary>\n",
    "A halo mass function is just a volume normalised histogram of the masses. Often done in logspace rather than linear space.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f9616d-2146-4e08-83f5-150560523967",
   "metadata": {},
   "source": [
    "## Linking in other particle types\n",
    "\n",
    "To simply combine the other particle types into these halos we will just attach baryonic particles to their nearest dark matter particle (as long as it's within the linking length). Obviously this won't be a particularly robust definition but its good enough.\n",
    "\n",
    "(Again remember to modify the file path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b44e52a-a213-425e-8d5c-fa3b8ab2c84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over our halos and construct the halo ID array\n",
    "for i in range(nhalo):\n",
    "    part_halo_ids[halo_indices[i]] = i\n",
    "\n",
    "# Now all we have to do is load up the other particle positions, query the DM tree for the DM neighbours and assign their halo\n",
    "hdf = h5py.File(\"../snapshots/EAGLE_6/snapshot_0002.hdf5\", \"r\")\n",
    "gas_pos = hdf[\"PartType0/Coordinates\"][:]\n",
    "star_pos = hdf[\"PartType4/Coordinates\"][:]\n",
    "bh_pos = hdf[\"PartType5/Coordinates\"][:]\n",
    "hdf.close()\n",
    "\n",
    "# Query the tree (this looking for only 1 neighbour)\n",
    "_, gas_inds = tree.query(gas_pos, k=1, distance_upper_bound=linkl)\n",
    "_, star_inds = tree.query(star_pos, k=1, distance_upper_bound=linkl)\n",
    "_, bh_inds = tree.query(bh_pos, k=1, distance_upper_bound=linkl)\n",
    "\n",
    "# Finally we can make a new dictionary with all particle types\n",
    "halo_catologue = {\"DM\": halo_indices, \"Gas\": {}, \"Stars\": {}, \"Black Holes\": {}}\n",
    "for i, ind in enumerate(gas_inds):\n",
    "\n",
    "    # Skip particles not in a halo\n",
    "    if ind == tree.n:\n",
    "        continue\n",
    "        \n",
    "    # Get the dark matter halo ID\n",
    "    dm_halo_id = part_halo_ids[ind]\n",
    "\n",
    "    # If it's a legit halo store it\n",
    "    if dm_halo_id >= 0:\n",
    "        halo_catologue[\"Gas\"].setdefault(dm_halo_id, set()).update({i, })\n",
    "        \n",
    "for i, ind in enumerate(star_inds):\n",
    "    \n",
    "    # Skip particles not in a halo\n",
    "    if ind == tree.n:\n",
    "        continue\n",
    "        \n",
    "    # Get the dark matter halo ID\n",
    "    dm_halo_id = part_halo_ids[ind]\n",
    "\n",
    "    # If it's a legit halo store it\n",
    "    dm_halo_id = part_halo_ids[ind]\n",
    "    if dm_halo_id >= 0:\n",
    "        halo_catologue[\"Stars\"].setdefault(dm_halo_id, set()).update({i, })\n",
    "        \n",
    "for i, ind in enumerate(bh_inds):\n",
    "\n",
    "    # Skip particles not in a halo\n",
    "    if ind == tree.n:\n",
    "        continue\n",
    "        \n",
    "    # Get the dark matter halo ID\n",
    "    dm_halo_id = part_halo_ids[ind]\n",
    "\n",
    "    # If it's a legit halo store it\n",
    "    dm_halo_id = part_halo_ids[ind]\n",
    "    if dm_halo_id >= 0:\n",
    "        halo_catologue[\"Black Holes\"].setdefault(dm_halo_id, set()).update({i, })\n",
    "\n",
    "# And convert all entries to arrays making empty ones when they don't appear\n",
    "for key in range(nhalo):\n",
    "    if key in halo_catologue[\"Gas\"]:\n",
    "        halo_catologue[\"Gas\"][key] = np.array(list(halo_catologue[\"Gas\"][key]))\n",
    "    else:\n",
    "        halo_catologue[\"Gas\"][key] = np.array([])\n",
    "        \n",
    "for key in range(nhalo):\n",
    "    if key in halo_catologue[\"Stars\"]:\n",
    "        halo_catologue[\"Stars\"][key] = np.array(list(halo_catologue[\"Stars\"][key]))\n",
    "    else:\n",
    "        halo_catologue[\"Stars\"][key] = np.array([])\n",
    "        \n",
    "for key in range(nhalo):\n",
    "    if key in halo_catologue[\"Black Holes\"]:\n",
    "        halo_catologue[\"Black Holes\"][key] = np.array(list(halo_catologue[\"Black Holes\"][key]))\n",
    "    else:\n",
    "        halo_catologue[\"Black Holes\"][key] = np.array([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4009601e-ed99-4901-b1b3-c6da71c50cc3",
   "metadata": {},
   "source": [
    "Once again we can now extract what ever particle properties we want using this catalogue of indices. Here is how we would calculate the stellar mass of each halo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e403402-3034-41a4-bd32-8d7e01398f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the stellar masses\n",
    "hdf = h5py.File(\"../snapshots/EAGLE_6/snapshot_0002.hdf5\", \"r\")\n",
    "star_ms = hdf[\"PartType4/Masses\"][:] * 10 ** 10\n",
    "hdf.close()\n",
    "\n",
    "# Create array to store halo massses\n",
    "nhalo = halo_id\n",
    "stellar_halo_ms = np.zeros(nhalo)\n",
    "\n",
    "# Loop over halos calculating DM mass\n",
    "for i in range(nhalo):\n",
    "    if halo_catologue[\"Stars\"][i].size > 0:\n",
    "        stellar_halo_ms[i] = np.sum(star_ms[halo_catologue[\"Stars\"][i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5051c-32f5-4dca-bc6e-164d989ee6ce",
   "metadata": {},
   "source": [
    "Now I've run out of ideas... What relations can you make with this catalogue? Can you plot a stellar mass-halo mass relation, the black hole-stellar mass relation? The star formation rate density function? Plot a phase diagram for the most massive halo? Etc etc etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd38f1-a2a7-4094-9758-f2ffcc11a7e8",
   "metadata": {},
   "source": [
    "### Finding galaxies\n",
    "\n",
    "If you want to find galaxies instead of host halos you can rerun the above halo finding process but redefine the linking length to be 0.1 $\\bar{x}$. This corresponds to an 8 times increase density to 1600 times the mean density of the Universe and is a good rough start to defining galaxies with a spatial FOF."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
